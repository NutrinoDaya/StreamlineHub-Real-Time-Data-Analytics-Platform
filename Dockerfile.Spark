FROM apache/spark:3.5.1-python3

# Switch to root user for package installation
USER root

# Install necessary packages
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    curl \
    vim \
    unzip \
    rsync \
    software-properties-common \
    ssh && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Set environment variables
ENV SPARK_HOME=/opt/spark \
    JAVA_HOME=/opt/java/openjdk \
    PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH:$JAVA_HOME/bin \
    HOME=/root

# Ensure Ivy can resolve dependencies
RUN mkdir -p /root/.ivy2 && chmod -R 777 /root/.ivy2

# Create project directories
RUN mkdir -p /opt/StreamLineHub/data \
    /opt/StreamLineHub/pipelines \
    /opt/StreamLineHub/config \
    /opt/spark/event_logs \
    /tmp/spark-worker

# Copy requirements and install dependencies
COPY spark-requirements.txt /tmp/
RUN pip3 install --upgrade pip && \
    pip3 install --no-cache-dir --timeout=300 --retries=5 -r /tmp/spark-requirements.txt

# Copy Spark configuration files
COPY dependencies/spark/spark-defaults.conf ${SPARK_HOME}/conf/
COPY dependencies/spark/spark-env.sh ${SPARK_HOME}/conf/
RUN chmod +x ${SPARK_HOME}/conf/spark-env.sh

# Copy start script
COPY bin/start-spark.sh /start-spark.sh
RUN chmod +x /start-spark.sh

# Ensure Spark directory permissions
RUN chmod -R 777 ${SPARK_HOME} /opt/spark/jars /opt/spark/event_logs /tmp/spark-worker

# Set project directory as an environment variable
ENV PROJECT_DIR="/opt/StreamLineHub"

# Set working directory
WORKDIR /opt/StreamLineHub

# Set entrypoint
ENTRYPOINT ["/start-spark.sh"]
